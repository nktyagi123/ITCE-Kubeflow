{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import kubeflow pipeline libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.components import func_to_container_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download data\n",
    "def download_data(download_link: str, data_path: OutputPath(str)):\n",
    "    import zipfile\n",
    "    import wget\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    # download files\n",
    "    wget.download(download_link.format(file='train'), f'{data_path}/train_csv.zip')\n",
    "    wget.download(download_link.format(file='test'), f'{data_path}/test_csv.zip')\n",
    "    \n",
    "    with zipfile.ZipFile(f\"{data_path}/train_csv.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "        \n",
    "    with zipfile.ZipFile(f\"{data_path}/test_csv.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_op = func_to_container_op(download_data,base_image=\"python:3.7\", packages_to_install=['wget', 'zipfile36'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path:comp.InputPath(str),load_data_path: comp.OutputPath()):\n",
    "    import pandas as pd\n",
    "    import os, pickle\n",
    "    \n",
    "    train_data_path = data_path + '/train.csv'\n",
    "    test_data_path = data_path + '/test.csv'\n",
    "    tweet_df= pd.read_csv(train_data_path)\n",
    "    test_df=pd.read_csv(test_data_path)\n",
    "    df=pd.concat([tweet_df,test_df])\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(load_data_path, exist_ok = True)\n",
    "    \n",
    "    # join train and test together\n",
    "    ntrain = tweet_df.shape[0]\n",
    "    ntest = test_df.shape[0]\n",
    "    with open(f'{load_data_path}/df', 'wb') as f:\n",
    "        pickle.dump((ntrain, df, tweet_df), f)\n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_step = func_to_container_op(\n",
    "    load_data,\n",
    "    output_component_file='load_data_component.yaml', # This is optional. It saves the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas','pickle5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(load_data_path:comp.InputPath(str), preprocess_data_path:comp.OutputPath(str)):\n",
    "    \n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import os, pickle\n",
    "    import string\n",
    "    #import spacy\n",
    "    #import gensim.downloader as api\n",
    "    \n",
    "    #nlp = spacy.load(\"en_core_web_sm\")\n",
    "    #wv = api.load('word2vec-google-news-300')\n",
    "    \n",
    "     #loading the train data\n",
    "    with open(f'{load_data_path}/df', 'rb') as f:\n",
    "        ntrain, df, tweet_df = pickle.load(f)\n",
    "        \n",
    "    def clean_text(text):\n",
    "        \n",
    "        '''\n",
    "            This method generates a clean text and returns a dataframe\n",
    "        '''\n",
    "        text = re.sub(r'(\\d{3})-(\\d{3})-(\\d{4})',' ', text)\n",
    "        text = re.sub(r'(\\d{2})/(\\d{2})',' ', text)\n",
    "        text = re.sub(r'\\S*@\\S*\\s?',' ', text)\n",
    "        text = re.sub(r'\\s+',' ', text)\n",
    "        text = re.sub(r'\\'',' ', text)\n",
    "        text = re.sub(r'<.*?>',' ', text)\n",
    "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*',' ', text)\n",
    "        text = re.sub(r'[^0-9a-z #+_]',' ', text)\n",
    "        text = re.sub(r'\\(.*?\\)',' ', text)\n",
    "        text = re.sub(r'(\\w+\\.\\w+\\.com)',' ', text)\n",
    "        text = re.sub(r'\\w+\\d+\\w+|\\w+\\d+|\\d+\\w+',' ', text)\n",
    "        text = re.sub(r'\\d+|\\@|\\*|\\=|\\+|\\$|\\%|\\.\\.|\\#|-|\\[|\\]|\\?|\\\"',' ', text)\n",
    "        text = re.sub(r'[ \\n]+',' ', text)\n",
    "        return text.strip().lower()\n",
    "    \n",
    "    # def preprocess(text):\n",
    "    #     doc = nlp(text)\n",
    "    #     filtered_token = []\n",
    "    #     for token in doc:\n",
    "    #         if token.is_stop or token.is_punct:\n",
    "    #             continue\n",
    "    #         filtered_token.append(token.lemma_)\n",
    "    #     return wv.get_mean_vector(filtered_token)\n",
    "\n",
    "    \n",
    "    df['text'] = df['Summary'].apply(clean_text)\n",
    "    #df['text'] = df['cleaned_txt'].apply(preprocess)\n",
    "    \n",
    "   \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(preprocess_data_path, exist_ok = True)\n",
    "    \n",
    "    with open(f'{preprocess_data_path}/df', 'wb') as f:\n",
    "        pickle.dump((ntrain, df, tweet_df), f)\n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data_step = func_to_container_op(preprocess_data, packages_to_install=['pandas', 'regex', 'pickle5'], output_component_file='preprocess_data_component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_creation(preprocess_data_path: comp.InputPath(str), corpus_path: comp.OutputPath(str)):\n",
    "    import os, pickle\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.util import ngrams\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    stop=set(stopwords.words('english'))\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    with open(f'{preprocess_data_path}/df', 'rb') as f:\n",
    "        ntrain, df, tweet_df = pickle.load(f)\n",
    "        \n",
    "    def create_corpus(df):\n",
    "        corpus=[]\n",
    "        for tweet in tqdm(df['text']):\n",
    "            words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "            corpus.append(words)\n",
    "        return corpus\n",
    "    \n",
    "     #creating the preprocess directory\n",
    "    os.makedirs(corpus_path, exist_ok = True)\n",
    "    \n",
    "    corpus=create_corpus(df)\n",
    "    with open(f'{corpus_path}/corpus', 'wb') as f:\n",
    "        pickle.dump((corpus,tweet_df), f)\n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_creation_step = func_to_container_op(corpus_creation, packages_to_install=['pandas', 'pickle5', 'nltk','tqdm'], output_component_file='corpus_creation_component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_step(download_link_glove: str,corpus_path: comp.InputPath(str), embedded_path: comp.OutputPath(str)):\n",
    "    \n",
    "    import os, pickle\n",
    "    import pandas as pd\n",
    "    import zipfile\n",
    "    import wget\n",
    "    import os\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.utils import pad_sequences\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    with open(f'{corpus_path}/corpus', 'rb') as f:\n",
    "        corpus, tweet_df = pickle.load(f)\n",
    "    \n",
    "    if not os.path.exists(embedded_path):\n",
    "        os.makedirs(embedded_path)\n",
    "    # download files\n",
    "    wget.download(download_link_glove, f'{embedded_path}/glove.6B.zip')\n",
    "    \n",
    "    with zipfile.ZipFile(f'{embedded_path}/glove.6B.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(embedded_path)\n",
    "    \n",
    "    embedding_dict={}\n",
    "    \"\"\"path_to_glove_file = os.path.join(\n",
    "        os.path.expanduser(\"~\"), f\"{embedded_path}/glove.6B.100d.txt\"\n",
    "    )\"\"\"\n",
    "    with open(f\"{embedded_path}/glove.6B.100d.txt\",'r') as f:\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            word=values[0]\n",
    "            vectors=np.asarray(values[1:],'float32')\n",
    "            embedding_dict[word]=vectors\n",
    "    f.close()\n",
    "    \n",
    "    MAX_NB_WORDS = 50000\n",
    "    MAX_LEN=250\n",
    "    tokenizer_obj=Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "    tokenizer_obj.fit_on_texts(corpus)\n",
    "    sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "    \n",
    "    tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "    word_index=tokenizer_obj.word_index\n",
    "    num_words=len(word_index)+1\n",
    "    embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "    for word,i in tqdm(word_index.items()):\n",
    "        if i > num_words:\n",
    "            continue\n",
    "\n",
    "        emb_vec=embedding_dict.get(word)\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[i]=emb_vec\n",
    "    \n",
    "    with open(f'{embedded_path}/embedding', 'wb') as f:\n",
    "        pickle.dump((embedding_matrix, num_words, tweet_pad, tweet_df, MAX_LEN), f)\n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_creation_step = func_to_container_op(embedding_step, packages_to_install=['pandas', 'zipfile36', 'wget','tqdm','keras','numpy','tensorflow', 'pickle5'], output_component_file='embedding_component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_building_and_training(embedded_path: comp.InputPath(str), model_path: comp.OutputPath(str)):\n",
    "    \n",
    "    import os, pickle;\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "    from keras.initializers import Constant\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping \n",
    "    \n",
    "    with open(f'{embedded_path}/embedding', 'rb') as f:\n",
    "        embedding_matrix, num_words, tweet_pad, tweet_df, MAX_LEN = pickle.load(f)\n",
    "    \n",
    "    train=tweet_pad[:tweet_df.shape[0]]\n",
    "    final_test=tweet_pad[tweet_df.shape[0]:]\n",
    "    X_train,X_test,y_train,y_test=train_test_split(train,tweet_df['label_num'].values,test_size=0.2)\n",
    "    \n",
    "    #defining model\n",
    "    model=Sequential()\n",
    "\n",
    "    embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                       input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "    model.add(embedding)\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(128, return_sequences=True,dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(64,dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    #Compiling the classifier model with Adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    #fitting the model\n",
    "    history=model.fit(X_train,y_train,batch_size=32,epochs=10,validation_data=(X_test,y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)],verbose=2)\n",
    "\n",
    "    #evaluate model\n",
    "    test_loss, test_acc = model.evaluate(np.array(X_test),  np.array(y_test), verbose=0)\n",
    "    print(\"Test_loss: {}, Test_accuracy: {} \".format(test_loss,test_acc))\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(model_path, exist_ok = True)\n",
    "    \n",
    "    #saving the model\n",
    "    model.save(f'{model_path}/model.h5')\n",
    "    \n",
    "    #dumping other values\n",
    "    with open(f'{model_path}/values', 'wb') as f:\n",
    "        pickle.dump((test_loss, test_acc), f)\n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_building_step = func_to_container_op(model_building_and_training, packages_to_install=['pandas', 'zipfile36', 'wget','tqdm','keras','numpy','tensorflow','scikit-learn','pickle5'], output_component_file='model_creation_component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='ITCE pipeline',\n",
    "    description='A simple pipeline for testing ITC Engine.',\n",
    ")\n",
    "def my_pipeline(\n",
    "    download_link: str = 'https://github.com/nktyagi123/kubeflow_itce/blob/main/{file}.csv.zip?raw=true',\n",
    "    data_path: str = \"/mnt\",\n",
    "    load_data_path: str = \"load\", \n",
    "    preprocess_data_path: str = \"preprocess\",\n",
    "    corpus_path: str = \"corpus\",\n",
    "    download_link_glove: str = \"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "    model_path: str = \"model\",\n",
    "):\n",
    "    download_container = download_op(download_link)\n",
    "    output1 = load_data_step(download_container.output)\n",
    "    output2 = preprocess_data_step(output1.output)\n",
    "    output3 = corpus_creation_step(output2.output)\n",
    "    output4 = embedding_creation_step(download_link_glove, output3.output)\n",
    "    output5 = model_building_step(output4.output)\n",
    "\n",
    "# Compile the pipeline\n",
    "kfp.compiler.Compiler().compile(my_pipeline, 'ITCE_Complete.yaml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "# replace download_link with the repo link where the data is stored https:github-repo/data-dir/{file}.csv.zip?raw=true\n",
    "download_link = 'https://github.com/nktyagi123/kubeflow_itce/blob/main/{file}.csv.zip?raw=true'\n",
    "data_path = \"/mnt\"\n",
    "load_data_path = \"load\"\n",
    "preprocess_data_path = \"preprocess\"\n",
    "corpus_path = \"corpus\",\n",
    "download_link_glove = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "model_path = \"model\"\n",
    "\n",
    "load_data_op = kfp.components.load_component_from_file('load_data_component.yaml')\n",
    "preprocess_average_op = kfp.components.load_component_from_file('preprocess_data_component.yaml')\n",
    "#save_result_op = kfp.components.load_component_from_file('save_result_component.yaml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (39024955.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[67], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    arguments = {\"download_link\": download_link,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "arguments = {\"download_link\": download_link,\n",
    "             \"data_path\": data_path,\n",
    "             \"load_data_path\": load_data_path,\n",
    "             \"preprocess_data_path\": preprocess_data_path,\n",
    "             \"corpus_path\":corpus_path,\n",
    "             \"download_link_glove\":download_link_glove,\n",
    "             \"model_path\":model_path,\n",
    "            }\n",
    "client.create_run_from_pipeline_func(trial_pipeline, arguments=arguments) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "docker_image": "",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": true,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:access-rok:true"
   ],
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "nlp-getting-started-vanilla-final-workspace-wwnkd",
     "size": 14,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
